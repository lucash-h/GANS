{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import time\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, LeakyReLU, Flatten, Reshape, BatchNormalization, Conv2DTranspose, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "batch_size = 32 \n",
    "epochs = 5\n",
    "latent_dimension = 100\n",
    "noise_dimension = 100\n",
    "generator_learning_rate = 0.001\n",
    "generator_learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict = {'generator_loss':[],\n",
    "             'discriminator_loss':[], \n",
    "             'real_discriminator_loss':[], \n",
    "             'fake_discriminator_loss':[],\n",
    "             'real_discriminator_acc':[],\n",
    "             'fake_discriminator_acc':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(real_images, real_labels), (test_set,test_set_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "assert real_images.shape == (60000, 28, 28)\n",
    "assert real_labels.shape == (60000,)\n",
    "\n",
    "assert test_set.shape == (10000, 28, 28)\n",
    "assert test_set_labels.shape == (10000,)\n",
    "\n",
    "\n",
    "\n",
    "real_images = real_images / 255.0\n",
    "test_set = test_set / 255.0\n",
    "\n",
    "real_images = tf.reshape(real_images,(-1,batch_size,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dimension):\n",
    "  xin = Input((latent_dimension))\n",
    "  xout = Dense(7*7*128, input_dim = latent_dimension)(xin) #map input noise to higher dimension\n",
    "  xout = Reshape((7,7,128))(xout) #reshape so suitable for convolution\n",
    "  xout = BatchNormalization()(xout) #normalizes\n",
    "  xout = LeakyReLU(alpha=0.2)(xout) #non linearity\n",
    "  xout = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same')(xout) #generate higher res\n",
    "  xout = BatchNormalization()(xout)\n",
    "  xout = LeakyReLU(alpha=0.2)(xout)\n",
    "  xout = Conv2DTranspose(1, (4, 4), strides=(2, 2), padding='same', activation='sigmoid')(xout) #tanh provides higher res -1 -> 1 rather than 0 -> 1 and vals are centered @ 0 for other layers\n",
    "  return Model(xin,xout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    xin = Input((28,28,1))\n",
    "    xout = Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=(28, 28, 1))(xin)\n",
    "    xout = LeakyReLU(alpha=0.2)(xout)\n",
    "    xout = Dropout(0.4)(xout)\n",
    "    xout = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(xout)\n",
    "    xout = LeakyReLU(alpha=0.2)(xout)\n",
    "    xout = Dropout(0.4)(xout)\n",
    "    xout = Flatten()(xout)\n",
    "    xout = Dense(1, activation='sigmoid')(xout) #sets vals 0-1 --> fake - real\n",
    "\n",
    "    return Model(xin,xout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator(latent_dimension)\n",
    "#generator.compile(optimizer='Adam', loss = 'bce')\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()\n",
    "#discriminator.compile(optimizer='Adam', loss='bce', metrics=['accuracy'])\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "gen_optimizer = tf.keras.optimizers.Adam(learning_rate=generator_learning_rate)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(learning_rate=discriminator_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "# Training loop\n",
    "  noise = tf.random.normal([batch_size, noise_dimension])\n",
    "  loss_log = {'generator_loss':[],'discriminator_loss':[], 'real_discriminator_loss':[], 'fake_discriminator_loss':[]} #, 'real_disc_acc':[], 'fake_disc_acc':[]}\n",
    "\n",
    "  with tf.GradientTape() as gen_tape:\n",
    "    generated_images = generator(noise)\n",
    "\n",
    "    fake_output = discriminator(generated_images)\n",
    "    generator_loss = bce(tf.ones(batch_size),fake_output)\n",
    "    loss_log['generator_loss'].append(generator_loss)\n",
    "\n",
    "\n",
    "  gradients_of_generator = gen_tape.gradient(generator_loss, generator.trainable_variables)\n",
    "  gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "  with tf.GradientTape() as disc_tape:\n",
    "    real_output = discriminator(images)\n",
    "    fake_output = discriminator(generated_images)\n",
    "\n",
    "    fake_discriminator_loss = bce(tf.zeros(batch_size),fake_output)\n",
    "    loss_log['fake_discriminator_loss'].append(fake_discriminator_loss)\n",
    "\n",
    "    real_discriminator_loss = bce(tf.ones(batch_size),real_output)\n",
    "    loss_log['real_discriminator_loss'].append(real_discriminator_loss)\n",
    "\n",
    "    total_discriminator_loss = fake_discriminator_loss + real_discriminator_loss\n",
    "    loss_log['discriminator_loss'].append(total_discriminator_loss)\n",
    "\n",
    "  gradients_of_discriminator = disc_tape.gradient(total_discriminator_loss, discriminator.trainable_variables)\n",
    "  disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "\n",
    "  return loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test set into batches of 32\n",
    "num_batches = len(test_set) // batch_size\n",
    "split_indices = [i * batch_size for i in range(1, num_batches)]\n",
    "test_set_batched = np.split(test_set, split_indices)\n",
    "\n",
    "print(len(test_set_batched))\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  for batch in tqdm(real_images):\n",
    "    batch_loss = train_step(batch)\n",
    "\n",
    "    for key in batch_loss:\n",
    "      loss_dict[key].append(batch_loss[key])\n",
    "  #add accuracy\n",
    "  batch_index = np.random.randint(0, 311) #chooses random int value from 0 to 311 to pull random batch from test set\n",
    "  result_real_eval = discriminator.evaluate(x=test_set_batched[batch_index],\n",
    "                                  y=tf.ones(32), \n",
    "                                  verbose = 0)\n",
    "  \n",
    "  result_gen_eval = discriminator.evaluate(x=generator(tf.random.normal([batch_size, noise_dimension])),\n",
    "                                  y=tf.zeros(32), \n",
    "                                  verbose = 0)\n",
    "  \n",
    "  loss_dict['real_discriminator_acc'].append(result_real_eval[1]) #index 0 is loss, index 1 is accuracy\n",
    "  loss_dict['fake_discriminator_acc'].append(result_gen_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(sample_number, images):\n",
    "\n",
    "    _edge_length = int(np.sqrt(sample_number))\n",
    "    fig,ax = plt.subplots(nrows = _edge_length, ncols = _edge_length,figsize=(10,10))\n",
    "    for i in range(_edge_length**2):\n",
    "        ax[i//_edge_length][i%_edge_length].imshow(images[i, :, :, 0])\n",
    "        ax[i//8][i%8].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_generated_samples = 64\n",
    "generated = generator(tf.random.normal((n_generated_samples,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(n_generated_samples, generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(list1,list1_title, list2, list2_title, downsample=False, ds_value=1, x_title='X', y_title='Y', plot_title='Plot'):\n",
    "\n",
    "    if downsample == True:\n",
    "        list1 = list1[::ds_value]\n",
    "        list2 = list2[::ds_value]\n",
    "\n",
    "    plt.figure().set_figwidth(15)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.xlabel(x_title)\n",
    "    plt.ylabel(y_title)\n",
    "    plt.title(plot_title)\n",
    "\n",
    "    plt.plot(list1, label=list1_key)\n",
    "    plt.plot(list2, label=list2_key)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(loss_dict['generator_loss'], \n",
    "              'Generator Loss', \n",
    "              loss_dict['discriminator_loss'], \n",
    "              'Discriminator Loss', \n",
    "              True, \n",
    "              10, \n",
    "              'X', \n",
    "              'Overall Loss', \n",
    "              'Generator vs Discriminator Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(loss_dict['real_discriminator_loss'], \n",
    "              'Real Discriminator Loss', \n",
    "              loss_dict['fake_discriminator_loss'], \n",
    "              'Fake Discriminator Loss', \n",
    "              True, \n",
    "              10, \n",
    "              'X', \n",
    "              'Discriminator Loss Quality', \n",
    "              'Real vs Fake Discriminator Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(loss_dict['real_discriminator_acc'],\n",
    "              'Real Discriminator Accuracy',\n",
    "              loss_dict['fake_discriminator_acc'],\n",
    "              'Fake Discriminator Accuracy',\n",
    "              False,\n",
    "              10,\n",
    "              'X',\n",
    "              'Discriminator Accuracy Quality',\n",
    "              'Real vs Fake Discriminator Accuracy')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
